{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .ns5 or .nf3 files must be stored in a folder called input in the same path as this Jupyter Notebook. The results will be saved in a folder called output in the same path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from core.parameter_functions.Parameters import par\n",
    "from scipy.io import loadmat\n",
    "import multiprocessing\n",
    "import sys ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Current notebook folder\n",
    "notebook_dir = Path().resolve()  \n",
    "\n",
    "# Add core folder\n",
    "core_path = notebook_dir / \"core\"\n",
    "sys.path.append(str(core_path))\n",
    "\n",
    "# Add new_processing_pipeline folder\n",
    "new_pipeline_path = notebook_dir.parent / \"codes_emu\" / \"codes_for_analysis\" / \"new_processing_pipeline\"\n",
    "sys.path.append(str(new_pipeline_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_ripple import parse_ripple\n",
    "from core.filtering import filtering\n",
    "from core.plot_continuous_bundles import PlotBundles\n",
    "from core.spikeDetection import Spikes\n",
    "from core.Collision import Collision\n",
    "from core.ArtifactRejection import ArtifactRejection\n",
    "from core.FeatureExtraction import FeatureExtractor\n",
    "from core.Clustering import SPCClustering\n",
    "from core.TemplateMatching import TemplateMatcher\n",
    "from core.SpikeMetrics import SpikeMetricsCalculator\n",
    "from core.RescueSpikes import SpikeRescuer\n",
    "from core.MergeClusters import ClusterMerger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and output directories\n",
    "input_path = Path('input')\n",
    "output_path = Path('output')\n",
    "\n",
    "for folder in [input_path, output_path]:\n",
    "    folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Check if input has data\n",
    "if not any(input_path.iterdir()):\n",
    "    print(\"Please place your raw data files in the 'input' folder\")\n",
    "else:\n",
    "    print(f\"Found {len(list(input_path.glob('*')))} file(s) in input folder\")\n",
    "    \n",
    "print(f\"\\nFolders ready:\")\n",
    "print(f\"  Input:  {input_path.resolve()}\")\n",
    "print(f\"  Output: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = par()\n",
    "param.parallel = True\n",
    "if param.micros:\n",
    "    binary_ext = '.NC5'\n",
    "    ext = '.ns5'\n",
    "else:\n",
    "    binary_ext = '.NC3'\n",
    "    ext = '.nf3'\n",
    "file_paths = glob.glob(\"input/*\"+binary_ext)\n",
    "if file_paths == []:\n",
    "    file_paths_ns5 = glob.glob(\"input/*\"+ext)\n",
    "    parse_ripple(file_paths_ns5)\n",
    "    file_paths = glob.glob(\"input/*\"+binary_ext)\n",
    "NSx_file_path = os.path.abspath(glob.glob(\"input/NSx.mat\")[0])\n",
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "dir_path = Path().resolve()\n",
    "#pics_used_dir = dir_path + '/input/pics_used'\n",
    "pics_used_dir = dir_path / 'input' / 'pics_used'\n",
    "metadata = loadmat(NSx_file_path)\n",
    "nsx = metadata['NSx']\n",
    "if param.micros:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['unit']=='uV')[1]) & set(np.where(nsx['sr']==30000)[1]))]\n",
    "else:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['sr']==2000)[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select and process specific channels for the rest of the pipeline, instead of all, edit the following block of code following the mentioned comments:\n",
    "\n",
    "specific_channels = 'all'  # Use all channels\n",
    "# specific_channels = [290, 291, 292, 293]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 channels: [array([257], dtype=object), array([263], dtype=object), array([264], dtype=object)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/core/filtering.py:311: RuntimeWarning: divide by zero encountered in log\n",
      "  flog_data1 = np.log(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete! Results saved to /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/process_info.mat\n"
     ]
    }
   ],
   "source": [
    "filter = filtering(save_fig=False,show_img=True,direc_resus_bae=str(Path().resolve()),\n",
    "                    resus_folder_name='spectra',direc_raw=str(Path().resolve()),with_NoNotch = False,\n",
    "                    time_plot_duration = 1,freq_line=60,parallel=param.parallel,k_periodograms=200,notch_filter=True,spectrum_resolution=0.5)\n",
    "\n",
    "# To select specific channels:\n",
    "# specific_channels = 'all'  # Use all channels\n",
    "specific_channels = [257, 263, 264]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "filter.new_check_lfp_power_NSX(metadata, channels = specific_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting all 68 channels\n",
      "Plotting bundle mLAMY with 8 channels: [array([[266]], dtype=uint16) array([[267]], dtype=uint16)\n",
      " array([[268]], dtype=uint16) array([[269]], dtype=uint16)\n",
      " array([[270]], dtype=uint16) array([[271]], dtype=uint16)\n",
      " array([[272]], dtype=uint16) array([[273]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLAMY_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mLFUS with 8 channels: [array([[289]], dtype=uint16) array([[290]], dtype=uint16)\n",
      " array([[291]], dtype=uint16) array([[292]], dtype=uint16)\n",
      " array([[293]], dtype=uint16) array([[294]], dtype=uint16)\n",
      " array([[295]], dtype=uint16) array([[296]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLFUS_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mLHIPH with 8 channels: [array([[298]], dtype=uint16) array([[299]], dtype=uint16)\n",
      " array([[300]], dtype=uint16) array([[301]], dtype=uint16)\n",
      " array([[302]], dtype=uint16) array([[303]], dtype=uint16)\n",
      " array([[304]], dtype=uint16) array([[305]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLHIPH_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mLTP with 8 channels: [array([[257]], dtype=uint16) array([[258]], dtype=uint16)\n",
      " array([[259]], dtype=uint16) array([[260]], dtype=uint16)\n",
      " array([[261]], dtype=uint16) array([[262]], dtype=uint16)\n",
      " array([[263]], dtype=uint16) array([[264]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLTP_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRAMY with 8 channels: [array([[330]], dtype=uint16) array([[331]], dtype=uint16)\n",
      " array([[332]], dtype=uint16) array([[333]], dtype=uint16)\n",
      " array([[334]], dtype=uint16) array([[335]], dtype=uint16)\n",
      " array([[336]], dtype=uint16) array([[337]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRAMY_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRFUS with 8 channels: [array([[362]], dtype=uint16) array([[363]], dtype=uint16)\n",
      " array([[364]], dtype=uint16) array([[365]], dtype=uint16)\n",
      " array([[366]], dtype=uint16) array([[367]], dtype=uint16)\n",
      " array([[368]], dtype=uint16) array([[369]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRFUS_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRHIPH with 8 channels: [array([[321]], dtype=uint16) array([[322]], dtype=uint16)\n",
      " array([[323]], dtype=uint16) array([[324]], dtype=uint16)\n",
      " array([[325]], dtype=uint16) array([[326]], dtype=uint16)\n",
      " array([[327]], dtype=uint16) array([[328]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRHIPH_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRTP with 8 channels: [array([[353]], dtype=uint16) array([[354]], dtype=uint16)\n",
      " array([[355]], dtype=uint16) array([[356]], dtype=uint16)\n",
      " array([[357]], dtype=uint16) array([[358]], dtype=uint16)\n",
      " array([[359]], dtype=uint16) array([[360]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRTP_4_filtorder1withnothesneg.png\n"
     ]
    }
   ],
   "source": [
    "plt = PlotBundles()\n",
    "\n",
    "# To select specific channels:\n",
    "specific_channels = 'all'  # Use all channels\n",
    "# specific_channels = [257, 263]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "plt.plot(nsx_file=nsx, par=param, channels=specific_channels, notchfilter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 specific channels: [array([257], dtype=object), array([263], dtype=object), array([264], dtype=object)]\n",
      "neg_thr_channels: [array([257], dtype=object), array([263], dtype=object), array([264], dtype=object)]\n",
      "pos_thr_channels: None\n",
      "\n",
      "Starting spike detection...\n",
      "Processing 3 channels with negative detection...\n",
      "Spike file already exists for channel 264, skipping...\n",
      "Spike file already exists for channel 263, skipping...\n",
      "Spike file already exists for channel 257, skipping...\n",
      "Spike detection done!\n"
     ]
    }
   ],
   "source": [
    "# Get all available channels\n",
    "if param.micros:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['unit']=='uV')[1]) & \n",
    "                                          set(np.where(nsx['sr']==30000)[1]))]\n",
    "else:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['sr']==2000)[1]))]\n",
    "\n",
    "# ========================================\n",
    "# CHANNEL SELECTION (NON-INTERACTIVE)\n",
    "# ========================================\n",
    "\n",
    "# Option 1: Use all channels\n",
    "# specific_channels = 'all'\n",
    "\n",
    "# Option 2: Use specific channels\n",
    "specific_channels = [257, 263, 264]\n",
    "\n",
    "# Option 3: Use single channel\n",
    "# specific_channels = 290\n",
    "\n",
    "# Option 4: Use range of channels\n",
    "# specific_channels = list(range(290, 301))  # 290-300\n",
    "\n",
    "# ========================================\n",
    "# DETECTION TYPE ASSIGNMENT\n",
    "# ========================================\n",
    "\n",
    "# Option 1: All channels use negative detection (default)\n",
    "neg_channels = 'all'\n",
    "pos_channels = None\n",
    "\n",
    "# Option 2: Specific channels for each detection type\n",
    "# neg_channels = [290, 291, 292]\n",
    "# pos_channels = [293, 294]\n",
    "# # Remaining channels will use 'both' detection\n",
    "\n",
    "# Option 3: All use negative, none use positive\n",
    "# neg_channels = 'all'\n",
    "# pos_channels = None\n",
    "\n",
    "# ========================================\n",
    "# PROCESS CHANNEL SELECTION\n",
    "# ========================================\n",
    "\n",
    "def parse_channels(channel_spec, all_channels):\n",
    "    \"\"\"Convert channel specification to channel objects.\"\"\"\n",
    "    if isinstance(channel_spec, str) and channel_spec.lower() == 'all':\n",
    "        return all_channels\n",
    "    elif channel_spec is None or (isinstance(channel_spec, list) and len(channel_spec) == 0):\n",
    "        return np.array([], dtype=object)\n",
    "    else:\n",
    "        if isinstance(channel_spec, int):\n",
    "            channel_spec = [channel_spec]\n",
    "        return np.array([ch for ch in all_channels if ch[0] in channel_spec], dtype=object)\n",
    "\n",
    "# Parse specific_channels\n",
    "if isinstance(specific_channels, str) and specific_channels.lower() == 'all':\n",
    "    channels = all_channels\n",
    "    print(f\"Processing all {len(channels)} channels: {[ch[0] for ch in channels]}\")\n",
    "else:\n",
    "    if isinstance(specific_channels, int):\n",
    "        specific_channels = [specific_channels]\n",
    "    channels = np.array([ch for ch in all_channels if ch[0] in specific_channels], dtype=object)\n",
    "    print(f\"Processing {len(channels)} specific channels: {[ch[0] for ch in channels]}\")\n",
    "\n",
    "# Parse neg_channels and pos_channels\n",
    "neg_thr_channels = parse_channels(neg_channels, channels)\n",
    "pos_thr_channels = parse_channels(pos_channels, channels)\n",
    "\n",
    "print(f\"neg_thr_channels: {[ch[0] for ch in neg_thr_channels] if len(neg_thr_channels) > 0 else 'None'}\")\n",
    "print(f\"pos_thr_channels: {[ch[0] for ch in pos_thr_channels] if len(pos_thr_channels) > 0 else 'None'}\")\n",
    "\n",
    "# ========================================\n",
    "# SETUP PARAMETERS\n",
    "# ========================================\n",
    "\n",
    "del param\n",
    "param = par()\n",
    "param.detection = 'neg'\n",
    "param.sr = 30000\n",
    "param.detect_fmin = 300\n",
    "param.detect_fmax = 3000\n",
    "param.auto = 0\n",
    "param.mVmin = 50\n",
    "param.w_pre = 20                       \n",
    "param.w_post = 44                     \n",
    "param.min_ref_per = 1.5                                    \n",
    "param.ref = np.floor(param.min_ref_per * param.sr / 1000)                  \n",
    "param.ref = param.ref\n",
    "param.factor_thr = 5\n",
    "param.detect_order = 4\n",
    "param.sort_order = 2\n",
    "param.detect_fmin = 300\n",
    "param.sort_fmin = 300\n",
    "param.stdmin = 5\n",
    "param.stdmax = 50\n",
    "param.ref_ms = 1.5\n",
    "param.preprocessing = True\n",
    "param.minus_one = 0\n",
    "param.interpolation = 'n'\n",
    "param.segments_length = 5\n",
    "param.tmax = 'all'\n",
    "param.cont_plot_samples = 60000\n",
    "\n",
    "# ========================================\n",
    "# RUN SPIKE DETECTION\n",
    "# ========================================\n",
    "\n",
    "print('\\nStarting spike detection...')\n",
    "param.detection = 'neg'\n",
    "\n",
    "if param.parallel:\n",
    "    spike = Spikes(par=param, nsx=nsx)\n",
    "    \n",
    "    # Process negative detection channels\n",
    "    if neg_thr_channels.size:\n",
    "        print(f\"Processing {len(neg_thr_channels)} channels with negative detection...\")\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "            pool.map(spike.get_spikes, neg_thr_channels, chunksize=1)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    \n",
    "    # Process positive detection channels\n",
    "    param.detection = 'pos'\n",
    "    if pos_thr_channels.size:\n",
    "        print(f\"Processing {len(pos_thr_channels)} channels with positive detection...\")\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "            pool.map(spike.get_spikes, pos_thr_channels, chunksize=1)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    \n",
    "    # Process 'both' detection channels\n",
    "    param.detection = 'both'\n",
    "    both_thr_channels = np.setdiff1d(\n",
    "        np.setdiff1d(\n",
    "            np.array([ch[0] for ch in channels]),\n",
    "            np.array([ch[0] for ch in neg_thr_channels])\n",
    "        ),\n",
    "        np.array([ch[0] for ch in pos_thr_channels]) if pos_thr_channels.size else []\n",
    "    )\n",
    "    \n",
    "    if both_thr_channels.size:\n",
    "        # Convert back to channel objects\n",
    "        both_thr_channels = np.array([ch for ch in channels if ch[0] in both_thr_channels], dtype=object)\n",
    "        print(f\"Processing {len(both_thr_channels)} channels with both detection...\")\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "            pool.map(spike.get_spikes, both_thr_channels, chunksize=1)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    \n",
    "    print('Spike detection done!')\n",
    "    \n",
    "else:\n",
    "    spike = Spikes(par=param, nsx=nsx)\n",
    "    \n",
    "    # Process negative detection channels\n",
    "    if neg_thr_channels.size:\n",
    "        print(f\"Processing {len(neg_thr_channels)} channels with negative detection...\")\n",
    "        for channel in neg_thr_channels:\n",
    "            spike.get_spikes(channel)\n",
    "    \n",
    "    # Process positive detection channels\n",
    "    param.detection = 'pos'\n",
    "    if pos_thr_channels.size:\n",
    "        print(f\"Processing {len(pos_thr_channels)} channels with positive detection...\")\n",
    "        for channel in pos_thr_channels:\n",
    "            spike.get_spikes(channel)\n",
    "    \n",
    "    # Process 'both' detection channels\n",
    "    param.detection = 'both'\n",
    "    both_thr_channels = np.setdiff1d(\n",
    "        np.setdiff1d(\n",
    "            np.array([ch[0] for ch in channels]),\n",
    "            np.array([ch[0] for ch in neg_thr_channels])\n",
    "        ),\n",
    "        np.array([ch[0] for ch in pos_thr_channels]) if pos_thr_channels.size else []\n",
    "    )\n",
    "    \n",
    "    if both_thr_channels.size:\n",
    "        # Convert back to channel objects\n",
    "        both_thr_channels = np.array([ch for ch in channels if ch[0] in both_thr_channels], dtype=object)\n",
    "        print(f\"Processing {len(both_thr_channels)} channels with both detection...\")\n",
    "        for channel in both_thr_channels:\n",
    "            spike.get_spikes(channel)\n",
    "    \n",
    "    print('Spike detection done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collisions for all 68 channels: [array([257], dtype=uint16), array([258], dtype=uint16), array([259], dtype=uint16), array([260], dtype=uint16), array([261], dtype=uint16), array([262], dtype=uint16), array([263], dtype=uint16), array([264], dtype=uint16), array([266], dtype=uint16), array([267], dtype=uint16), array([268], dtype=uint16), array([269], dtype=uint16), array([270], dtype=uint16), array([271], dtype=uint16), array([272], dtype=uint16), array([273], dtype=uint16), array([289], dtype=uint16), array([290], dtype=uint16), array([291], dtype=uint16), array([292], dtype=uint16), array([293], dtype=uint16), array([294], dtype=uint16), array([295], dtype=uint16), array([296], dtype=uint16), array([298], dtype=uint16), array([299], dtype=uint16), array([300], dtype=uint16), array([301], dtype=uint16), array([302], dtype=uint16), array([303], dtype=uint16), array([304], dtype=uint16), array([305], dtype=uint16), array([321], dtype=uint16), array([322], dtype=uint16), array([323], dtype=uint16), array([324], dtype=uint16), array([325], dtype=uint16), array([326], dtype=uint16), array([327], dtype=uint16), array([328], dtype=uint16), array([330], dtype=uint16), array([331], dtype=uint16), array([332], dtype=uint16), array([333], dtype=uint16), array([334], dtype=uint16), array([335], dtype=uint16), array([336], dtype=uint16), array([337], dtype=uint16), array([353], dtype=uint16), array([354], dtype=uint16), array([355], dtype=uint16), array([356], dtype=uint16), array([357], dtype=uint16), array([358], dtype=uint16), array([359], dtype=uint16), array([360], dtype=uint16), array([362], dtype=uint16), array([363], dtype=uint16), array([364], dtype=uint16), array([365], dtype=uint16), array([366], dtype=uint16), array([367], dtype=uint16), array([368], dtype=uint16), array([369], dtype=uint16), array([10241], dtype=uint16), array([10242], dtype=uint16), array([10245], dtype=uint16), array([10246], dtype=uint16)]\n",
      "Collision detection (['mLAMY']). Total artifacts:5489/144931(3.79%) \n",
      "\n",
      "Collision detection (['mLFUS']). Total artifacts:764/135179(0.57%) \n",
      "\n",
      "Collision detection (['mLHIPH']). Total artifacts:713/160612(0.44%) \n",
      "\n",
      "Collision detection (['mLTP']). Total artifacts:2086/70147(2.97%) \n",
      "\n",
      "Collision detection (['mRAMY']). Total artifacts:18104/154830(11.69%) \n",
      "\n",
      "Collision detection (['mRFUS']). Total artifacts:332/170907(0.19%) \n",
      "\n",
      "Collision detection (['mRHIPH']). Total artifacts:718/43226(1.66%) \n",
      "\n",
      "Collision detection (['mRTP']). Total artifacts:1657/38819(4.27%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = Collision(all_channels,nsx)\n",
    "\n",
    "# To select specific channels:\n",
    "specific_channels = 'all'  # Use all channels\n",
    "# specific_channels = [257, 263]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "col.separate_collisions(channels=specific_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected channels: [257, 263]\n",
      "Found 2 channels to process\n",
      "Thresholds: Prom>2.0, Width>3.0 & <15.0\n",
      "Low Amp Percentile: 5.0%, Other Prom Min: 1.0%\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing channel 257: mLTP01 raw_257_spikes.mat\n",
      "============================================================\n",
      "Loaded 24007 spikes (using spikes)\n",
      "Calculating metrics for 24007 spikes...\n",
      "  Using 80 CPU cores for parallel processing...\n",
      "Applying artifact rejection logic...\n",
      "  Amplitude threshold (5.0th percentile): 22.06\n",
      "  Auto-quarantined (bad ratio/width): 81\n",
      "  Low amplitude: 1201\n",
      "  One-peak exceptions preserved: 23\n",
      "\n",
      "Results:\n",
      "  Total spikes: 24007\n",
      "  Preserved: 19455 (81.0%)\n",
      "  Quarantined: 4462 (18.6%)\n",
      "  Bundle collision: 296 (1.2%)\n",
      "  Both quarantined & bundle: 206\n",
      "Saved updated file to: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/mLTP01 raw_257_spikes.mat\n",
      "\n",
      "============================================================\n",
      "Processing channel 263: mLTP07 raw_263_spikes.mat\n",
      "============================================================\n",
      "Loaded 7425 spikes (using spikes)\n",
      "Calculating metrics for 7425 spikes...\n",
      "  Using 80 CPU cores for parallel processing...\n",
      "Applying artifact rejection logic...\n",
      "  Amplitude threshold (5.0th percentile): 16.28\n",
      "  Auto-quarantined (bad ratio/width): 103\n",
      "  Low amplitude: 372\n",
      "  One-peak exceptions preserved: 30\n",
      "\n",
      "Results:\n",
      "  Total spikes: 7425\n",
      "  Preserved: 4869 (65.6%)\n",
      "  Quarantined: 2437 (32.8%)\n",
      "  Bundle collision: 351 (4.7%)\n",
      "  Both quarantined & bundle: 232\n",
      "Saved updated file to: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/mLTP07 raw_263_spikes.mat\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "artifact_rej = ArtifactRejection(\n",
    "    input_dir=str(Path().resolve() / 'output'),\n",
    "    output_dir=None,  # None = same as input_dir\n",
    "    backup_dir=None   # None = input_dir/backup\n",
    ")\n",
    "\n",
    "# Set thresholds (optional - defaults shown)\n",
    "artifact_rej.set_thresholds(\n",
    "    prom_ratio=2.0,           # Prominence ratio threshold\n",
    "    width_upper=15.0,         # Upper width limit\n",
    "    width_lower=3.0,          # Lower width limit\n",
    "    low_amp_percentile=5.0,   # Low amplitude percentile\n",
    "    other_prom_min_ratio=0.01 # 1% rule for other prominence\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# CHANNEL SELECTION\n",
    "# ========================================\n",
    "\n",
    "# To select specific channels:\n",
    "# specific_channels = 'all'  # Use all channels\n",
    "specific_channels = [257, 263]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "# ========================================\n",
    "# PROCESS ARTIFACT REJECTION\n",
    "# ========================================\n",
    "\n",
    "artifact_rej.process_all_channels(channels=specific_channels, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "FEATURE EXTRACTION PIPELINE\n",
      "######################################################################\n",
      "Method: GMM\n",
      "Input directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output\n",
      "Output directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/features\n",
      "Channels: [304]\n",
      "Total channels: 1\n",
      "Scales: 4, Min weight: 0.005, Min coeff: 5\n",
      "######################################################################\n",
      "\n",
      "[1/1] Processing channel 304...\n",
      "\n",
      "============================================================\n",
      "Channel 304 - GMM extraction\n",
      "============================================================\n",
      "Loaded 2097 spikes (shape: (2097, 64))\n",
      "  Running GMM_1channel (scales=4)...\n",
      "  Processed 64 coefficients\n",
      "  Filtering components (min_weight=0.005)...\n",
      "  Expanding metrics into vectors...\n",
      "  Applying knee detection...\n",
      "  Selecting final coefficients...\n",
      "  Selected 10 coefficients\n",
      "Saved features to: ch304_features_gmm.mat\n",
      "Feature matrix shape: (2097, 10)\n",
      "\n",
      "######################################################################\n",
      "PROCESSING COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "Output files: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/features\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define input/output directories\n",
    "input_dir = str(Path().resolve() / 'output')\n",
    "output_dir = str(Path().resolve() / 'output' / 'features')\n",
    "\n",
    "# GMM (recommended)\n",
    "extractor = FeatureExtractor(input_dir, output_dir, method='gmm')\n",
    "#extractor.process_all_channels(channels='all')\n",
    "extractor.process_all_channels(channels=[304])\n",
    "\n",
    "# # Haar (fast)\n",
    "# extractor = FeatureExtractor(input_dir, output_dir, method='haar')\n",
    "# extractor.process_all_channels(channels=[257, 263])\n",
    "\n",
    "# # PCA\n",
    "# extractor = FeatureExtraction(input_dir, output_dir, method='pca', n_components=10)\n",
    "# extractor.process_all_channels(channels=list(range(290, 301)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "SPC CLUSTERING PIPELINE\n",
      "######################################################################\n",
      "Input directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/features\n",
      "Output directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/features\n",
      "Times output directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output\n",
      "Channels: [328]\n",
      "Total channels: 1\n",
      "Parameters:\n",
      "  Min cluster size: 20\n",
      "  Max clusters: 20\n",
      "  Temperature: 0.15\n",
      "  K-nearest neighbors: 11\n",
      "  Generate times files: True\n",
      "  Method: Fallback SPC\n",
      "######################################################################\n",
      "\n",
      "[1/1] Processing channel 328...\n",
      "\n",
      "============================================================\n",
      "Channel 328 - SPC Clustering\n",
      "File: ch328_features_gmm.mat (feature file)\n",
      "============================================================\n",
      "Loaded 82 spikes with 10 features\n",
      "  Computing pairwise distances...\n",
      "  Finding 11 nearest neighbors...\n",
      "  Finding connected components...\n",
      "  Filtering clusters by size (min_clus=20)...\n",
      "  Found 0 clusters, 82 noise spikes\n",
      "Saved clustering results to: ch328_features_gmm.mat\n",
      "Clusters: 0\n",
      "  Noise: 82 spikes\n",
      "Generating times file...\n",
      "  ✓ Times file created: times_ch328.mat\n",
      "\n",
      "######################################################################\n",
      "CLUSTERING COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "Output directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output/features\n",
      "Times files saved to: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cluster all channels in feature directory\n",
    "clusterer = SPCClustering(\n",
    "    input_dir=str(Path().resolve() / 'output' / 'features'),\n",
    "    times_output_dir=str(Path().resolve() / 'output'),  # Explicitly set output dir\n",
    "    min_clus=20,\n",
    "    temperature=0.15,\n",
    "    knn=11,\n",
    "    generate_times_files=True,  # Explicitly enable\n",
    "    verbose=True  # Show all logs\n",
    ")\n",
    "#clusterer.process_all_channels(channels='all')\n",
    "\n",
    "# Cluster specific channels\n",
    "clusterer.process_all_channels(channels=[328])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "TEMPLATE MATCHING PIPELINE\n",
      "######################################################################\n",
      "Input directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output\n",
      "Output directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output\n",
      "Total channels to process: 1\n",
      "Parameters: {'template_sdnum': 3.0, 'template_k': None, 'template_k_min': 1, 'use_pointdist': False, 'pointlimit': None, 'metric': 'euclidean'}\n",
      "######################################################################\n",
      "\n",
      "\n",
      "============================================================\n",
      "Channel 328 - Template Matching\n",
      "File: times_ch328.mat\n",
      "============================================================\n",
      "Skipping: No classified spikes found to build templates.\n",
      "\n",
      "######################################################################\n",
      "TEMPLATE MATCHING COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "Output directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the matcher\n",
    "matcher = TemplateMatcher(\n",
    "                    input_dir=str(Path().resolve() / 'output'),\n",
    "                    template_sdnum=3.0,\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "                \n",
    "## Match unclassified spikes in ALL channels\n",
    "# matcher.process_all_channels(channels='all')\n",
    "                \n",
    "# Match unclassified spikes in specific channels\n",
    "matcher.process_all_channels(channels=[328])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_calc = SpikeMetricsCalculator(\n",
    "        input_dir=str(Path().resolve() / 'output'),\n",
    "        output_dir=str(Path().resolve() / 'output/Quality_Metrics')\n",
    "    )\n",
    "    \n",
    "## Run on all channels\n",
    "#metrics_calc.process_all_channels(channels='all')\n",
    "    \n",
    "## Run on specific channels\n",
    "metrics_calc.process_all_channels(channels=[328])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the directories\n",
    "# This assumes your notebook is in 'full_processing_pipeline'\n",
    "base_dir = Path().resolve() \n",
    "times_dir = str(base_dir / 'output')\n",
    "spikes_dir = str(base_dir / 'output')\n",
    "\n",
    "# 3. Initialize the SpikeRescuer\n",
    "# You can choose 'euclidean' or 'correlation'\n",
    "rescuer = SpikeRescuer(\n",
    "    times_dir=times_dir,\n",
    "    spikes_dir=spikes_dir,\n",
    "    output_dir=times_dir,  # This will update the times_*.mat files in-place\n",
    "    template_sdnum=3.0,    # Use 3.0 for 'euclidean'\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "# --- 4. Run the rescue operation ---\n",
    "\n",
    "# Option A: Run on ALL channels\n",
    "# This will find all channels that have both a 'times_*.mat'\n",
    "# and a '*_spikes.mat' file and process them.\n",
    "# print(\"--- Running Spike Rescue on ALL channels ---\")\n",
    "#rescuer.process_all_channels(channels='all')\n",
    "\n",
    "\n",
    "# Option B: Run on specific channels\n",
    "print(\"\\n--- Running Spike Rescue on specific channels ---\")\n",
    "specific_channels = [257]\n",
    "rescuer.process_all_channels(channels=specific_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output_merged_mat\n",
      "Created directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output_merged_reports\n",
      "\n",
      "######################################################################\n",
      "CLUSTER MERGING & REPORTING PIPELINE\n",
      "######################################################################\n",
      "Input directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output\n",
      "Output MAT directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output_merged_mat\n",
      "Output PDF directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output_merged_reports\n",
      "Total channels to process: 1\n",
      "Merge Groups: [[1, 3], [2, 5, 7]]\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Channel 328 - Cluster Merging\n",
      "File: times_ch328.mat\n",
      "======================================================================\n",
      "\n",
      "=� Original cluster distribution:\n",
      "   Cluster 0:    82 spikes (100.0%)\n",
      "\n",
      "= Applying merges...\n",
      "\n",
      "   Merging [1, 3] � 1\n",
      "     Cluster 3 not found (0 spikes)\n",
      "   Total merged into cluster 1: 0 spikes\n",
      "\n",
      "   Merging [2, 5, 7] � 2\n",
      "     Cluster 5 not found (0 spikes)\n",
      "     Cluster 7 not found (0 spikes)\n",
      "   Total merged into cluster 2: 0 spikes\n",
      "\n",
      "= Renumbering clusters to be contiguous (0 to N)...\n",
      "   Cluster mapping (old � new):\n",
      "       0 �   0   (   82 spikes)\n",
      "\n",
      "= Final cluster distribution:\n",
      "   Cluster 0:    82 spikes (100.0%)\n",
      "\n",
      " Cluster count: 1 � 1\n",
      "\n",
      "Saved merged data to: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output_merged_mat/times_ch328_merged.mat\n",
      "\n",
      "=� Generating cluster report for merged data...\n",
      "--- FAKE make_cluster_report ---\n",
      "   No figures generated by report.\n",
      "\n",
      "######################################################################\n",
      "MERGE & REPORT COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "Output directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output_merged_mat\n",
      "Report directory: /media/sEEG_DATA/Tests/Matlab sorting pipeline/Tapasi/MCWs_Pipeline/full_processing_pipeline/output_merged_reports\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ClusterMerger\n",
    "merger = ClusterMerger(\n",
    "    input_dir=str(Path().resolve() / 'output'),\n",
    "    output_dir=str(Path().resolve() / 'output_merged_mat'),\n",
    "    report_dir=str(Path().resolve() / 'output_merged_reports'),\n",
    "    calc_metrics=True,\n",
    "    verbose=True,\n",
    "    clusters_per_page=6  # Optional: kwargs for make_cluster_report\n",
    ")\n",
    "\n",
    "# Define merge groups (optional)\n",
    "# Each sublist: [target_cluster, source_cluster1, source_cluster2, ...]\n",
    "merge_rules = [\n",
    "    [1, 3],    # Merge cluster 3 into cluster 1\n",
    "    [2, 5, 7]  # Merge clusters 5 and 7 into cluster 2\n",
    "]\n",
    "\n",
    "# Run on all channels\n",
    "# merger.process_all_channels(channels='all', merge_groups=merge_rules)\n",
    "\n",
    "# Run on specific channels with merging\n",
    "merger.process_all_channels(channels=[328], merge_groups=merge_rules)\n",
    "\n",
    "# Or run without merging (just renumber clusters)\n",
    "# merger.process_all_channels(channels=[328], merge_groups=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
