{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .ns5 or .nf3 files must be stored in a folder called input in the same path as this Jupyter Notebook. The results will be saved in a folder called output in the same path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from core.parameter_functions.Parameters import par\n",
    "from scipy.io import loadmat\n",
    "import multiprocessing\n",
    "import sys ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Current notebook folder\n",
    "notebook_dir = Path().resolve()  \n",
    "\n",
    "# Add core folder\n",
    "core_path = notebook_dir / \"core\"\n",
    "sys.path.append(str(core_path))\n",
    "\n",
    "# Add new_processing_pipeline folder\n",
    "new_pipeline_path = notebook_dir.parent / \"codes_emu\" / \"codes_for_analysis\" / \"new_processing_pipeline\"\n",
    "sys.path.append(str(new_pipeline_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_ripple import parse_ripple\n",
    "from core.filtering import filtering\n",
    "from core.plot_continuous_bundles import PlotBundles\n",
    "from core.spikeDetection import Spikes\n",
    "from core.Collision import Collision\n",
    "from core.ArtifactRejection import ArtifactRejection\n",
    "from core.FeatureExtraction import FeatureExtractor\n",
    "from core.Clustering import SPCClustering\n",
    "from core.TemplateMatching import TemplateMatcher\n",
    "from core.SpikeMetrics import SpikeMetricsCalculator\n",
    "from core.RescueSpikes import SpikeRescuer\n",
    "from core.MergeClusters import ClusterMerger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 file(s) in input folder\n",
      "\n",
      "Folders ready:\n",
      "  Input:  /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/input\n",
      "  Output: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n"
     ]
    }
   ],
   "source": [
    "# Create input and output directories\n",
    "input_path = Path('input')\n",
    "output_path = Path('output')\n",
    "\n",
    "for folder in [input_path, output_path]:\n",
    "    folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Check if input has data\n",
    "if not any(input_path.iterdir()):\n",
    "    print(\"Please place your raw data files in the 'input' folder\")\n",
    "else:\n",
    "    print(f\"Found {len(list(input_path.glob('*')))} file(s) in input folder\")\n",
    "    \n",
    "print(f\"\\nFolders ready:\")\n",
    "print(f\"  Input:  {input_path.resolve()}\")\n",
    "print(f\"  Output: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = par()\n",
    "param.parallel = True\n",
    "if param.micros:\n",
    "    binary_ext = '.NC5'\n",
    "    ext = '.ns5'\n",
    "else:\n",
    "    binary_ext = '.NC3'\n",
    "    ext = '.nf3'\n",
    "file_paths = glob.glob(\"input/*\"+binary_ext)\n",
    "if file_paths == []:\n",
    "    file_paths_ns5 = glob.glob(\"input/*\"+ext)\n",
    "    parse_ripple(file_paths_ns5)\n",
    "    file_paths = glob.glob(\"input/*\"+binary_ext)\n",
    "NSx_file_path = os.path.abspath(glob.glob(\"input/NSx.mat\")[0])\n",
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "dir_path = Path().resolve()\n",
    "#pics_used_dir = dir_path + '/input/pics_used'\n",
    "pics_used_dir = dir_path / 'input' / 'pics_used'\n",
    "metadata = loadmat(NSx_file_path)\n",
    "nsx = metadata['NSx']\n",
    "if param.micros:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['unit']=='uV')[1]) & set(np.where(nsx['sr']==30000)[1]))]\n",
    "else:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['sr']==2000)[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select and process specific channels for the rest of the pipeline, instead of all, edit the following block of code following the mentioned comments:\n",
    "\n",
    "specific_channels = 'all'  # Use all channels\n",
    "# specific_channels = [290, 291, 292, 293]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 channels: [array([257], dtype=object), array([263], dtype=object), array([301], dtype=object)]\n",
      "\n",
      "Processing complete! Results saved to /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/process_info.mat\n"
     ]
    }
   ],
   "source": [
    "filter = filtering(save_fig=False,show_img=True,direc_resus_bae=str(Path().resolve()),\n",
    "                    resus_folder_name='spectra',direc_raw=str(Path().resolve()),with_NoNotch = False,\n",
    "                    time_plot_duration = 1,freq_line=60,parallel=param.parallel,k_periodograms=200,notch_filter=True,spectrum_resolution=0.5)\n",
    "\n",
    "# To select specific channels:\n",
    "# specific_channels = 'all'  # Use all channels\n",
    "specific_channels = [257, 263, 301]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "filter.new_check_lfp_power_NSX(metadata, channels = specific_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting all 68 channels\n",
      "Plotting bundle mLAMY with 8 channels: [array([[266]], dtype=uint16) array([[267]], dtype=uint16)\n",
      " array([[268]], dtype=uint16) array([[269]], dtype=uint16)\n",
      " array([[270]], dtype=uint16) array([[271]], dtype=uint16)\n",
      " array([[272]], dtype=uint16) array([[273]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLAMY_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mLFUS with 8 channels: [array([[289]], dtype=uint16) array([[290]], dtype=uint16)\n",
      " array([[291]], dtype=uint16) array([[292]], dtype=uint16)\n",
      " array([[293]], dtype=uint16) array([[294]], dtype=uint16)\n",
      " array([[295]], dtype=uint16) array([[296]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLFUS_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mLHIPH with 8 channels: [array([[298]], dtype=uint16) array([[299]], dtype=uint16)\n",
      " array([[300]], dtype=uint16) array([[301]], dtype=uint16)\n",
      " array([[302]], dtype=uint16) array([[303]], dtype=uint16)\n",
      " array([[304]], dtype=uint16) array([[305]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLHIPH_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mLTP with 8 channels: [array([[257]], dtype=uint16) array([[258]], dtype=uint16)\n",
      " array([[259]], dtype=uint16) array([[260]], dtype=uint16)\n",
      " array([[261]], dtype=uint16) array([[262]], dtype=uint16)\n",
      " array([[263]], dtype=uint16) array([[264]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mLTP_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRAMY with 8 channels: [array([[330]], dtype=uint16) array([[331]], dtype=uint16)\n",
      " array([[332]], dtype=uint16) array([[333]], dtype=uint16)\n",
      " array([[334]], dtype=uint16) array([[335]], dtype=uint16)\n",
      " array([[336]], dtype=uint16) array([[337]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRAMY_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRFUS with 8 channels: [array([[362]], dtype=uint16) array([[363]], dtype=uint16)\n",
      " array([[364]], dtype=uint16) array([[365]], dtype=uint16)\n",
      " array([[366]], dtype=uint16) array([[367]], dtype=uint16)\n",
      " array([[368]], dtype=uint16) array([[369]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRFUS_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRHIPH with 8 channels: [array([[321]], dtype=uint16) array([[322]], dtype=uint16)\n",
      " array([[323]], dtype=uint16) array([[324]], dtype=uint16)\n",
      " array([[325]], dtype=uint16) array([[326]], dtype=uint16)\n",
      " array([[327]], dtype=uint16) array([[328]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRHIPH_4_filtorder1withnothesneg.png\n",
      "Plotting bundle mRTP with 8 channels: [array([[353]], dtype=uint16) array([[354]], dtype=uint16)\n",
      " array([[355]], dtype=uint16) array([[356]], dtype=uint16)\n",
      " array([[357]], dtype=uint16) array([[358]], dtype=uint16)\n",
      " array([[359]], dtype=uint16) array([[360]], dtype=uint16)]\n",
      "Saved: z:\\Tests\\Matlab sorting pipeline\\Tapasi\\MCWs_Pipeline\\full_processing_pipeline/output/mRTP_4_filtorder1withnothesneg.png\n"
     ]
    }
   ],
   "source": [
    "plt = PlotBundles()\n",
    "\n",
    "# To select specific channels:\n",
    "specific_channels = 'all'  # Use all channels\n",
    "# specific_channels = [257, 263]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "plt.plot(nsx_file=nsx, par=param, channels=specific_channels, notchfilter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 specific channels: [array([257], dtype=object), array([263], dtype=object), array([264], dtype=object)]\n",
      "neg_thr_channels: [array([257], dtype=object), array([263], dtype=object), array([264], dtype=object)]\n",
      "pos_thr_channels: None\n",
      "\n",
      "Starting spike detection...\n",
      "Processing 3 channels with negative detection...\n",
      "Spike file already exists for channel 264, skipping...\n",
      "Spike file already exists for channel 263, skipping...\n",
      "Spike file already exists for channel 257, skipping...\n",
      "Spike detection done!\n"
     ]
    }
   ],
   "source": [
    "# Get all available channels\n",
    "if param.micros:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['unit']=='uV')[1]) & \n",
    "                                          set(np.where(nsx['sr']==30000)[1]))]\n",
    "else:\n",
    "    all_channels = nsx['chan_ID'][0][list(set(np.where(nsx['sr']==2000)[1]))]\n",
    "\n",
    "# ========================================\n",
    "# CHANNEL SELECTION (NON-INTERACTIVE)\n",
    "# ========================================\n",
    "\n",
    "# Option 1: Use all channels\n",
    "# specific_channels = 'all'\n",
    "\n",
    "# Option 2: Use specific channels\n",
    "specific_channels = [257, 263, 264]\n",
    "\n",
    "# Option 3: Use single channel\n",
    "# specific_channels = 290\n",
    "\n",
    "# Option 4: Use range of channels\n",
    "# specific_channels = list(range(290, 301))  # 290-300\n",
    "\n",
    "# ========================================\n",
    "# DETECTION TYPE ASSIGNMENT\n",
    "# ========================================\n",
    "\n",
    "# Option 1: All channels use negative detection (default)\n",
    "neg_channels = 'all'\n",
    "pos_channels = None\n",
    "\n",
    "# Option 2: Specific channels for each detection type\n",
    "# neg_channels = [290, 291, 292]\n",
    "# pos_channels = [293, 294]\n",
    "# # Remaining channels will use 'both' detection\n",
    "\n",
    "# Option 3: All use negative, none use positive\n",
    "# neg_channels = 'all'\n",
    "# pos_channels = None\n",
    "\n",
    "# ========================================\n",
    "# PROCESS CHANNEL SELECTION\n",
    "# ========================================\n",
    "\n",
    "def parse_channels(channel_spec, all_channels):\n",
    "    \"\"\"Convert channel specification to channel objects.\"\"\"\n",
    "    if isinstance(channel_spec, str) and channel_spec.lower() == 'all':\n",
    "        return all_channels\n",
    "    elif channel_spec is None or (isinstance(channel_spec, list) and len(channel_spec) == 0):\n",
    "        return np.array([], dtype=object)\n",
    "    else:\n",
    "        if isinstance(channel_spec, int):\n",
    "            channel_spec = [channel_spec]\n",
    "        return np.array([ch for ch in all_channels if ch[0] in channel_spec], dtype=object)\n",
    "\n",
    "# Parse specific_channels\n",
    "if isinstance(specific_channels, str) and specific_channels.lower() == 'all':\n",
    "    channels = all_channels\n",
    "    print(f\"Processing all {len(channels)} channels: {[ch[0] for ch in channels]}\")\n",
    "else:\n",
    "    if isinstance(specific_channels, int):\n",
    "        specific_channels = [specific_channels]\n",
    "    channels = np.array([ch for ch in all_channels if ch[0] in specific_channels], dtype=object)\n",
    "    print(f\"Processing {len(channels)} specific channels: {[ch[0] for ch in channels]}\")\n",
    "\n",
    "# Parse neg_channels and pos_channels\n",
    "neg_thr_channels = parse_channels(neg_channels, channels)\n",
    "pos_thr_channels = parse_channels(pos_channels, channels)\n",
    "\n",
    "print(f\"neg_thr_channels: {[ch[0] for ch in neg_thr_channels] if len(neg_thr_channels) > 0 else 'None'}\")\n",
    "print(f\"pos_thr_channels: {[ch[0] for ch in pos_thr_channels] if len(pos_thr_channels) > 0 else 'None'}\")\n",
    "\n",
    "# ========================================\n",
    "# SETUP PARAMETERS\n",
    "# ========================================\n",
    "\n",
    "del param\n",
    "param = par()\n",
    "param.detection = 'neg'\n",
    "param.sr = 30000\n",
    "param.detect_fmin = 300\n",
    "param.detect_fmax = 3000\n",
    "param.auto = 0\n",
    "param.mVmin = 50\n",
    "param.w_pre = 20                       \n",
    "param.w_post = 44                     \n",
    "param.min_ref_per = 1.5                                    \n",
    "param.ref = np.floor(param.min_ref_per * param.sr / 1000)                  \n",
    "param.ref = param.ref\n",
    "param.factor_thr = 5\n",
    "param.detect_order = 4\n",
    "param.sort_order = 2\n",
    "param.detect_fmin = 300\n",
    "param.sort_fmin = 300\n",
    "param.stdmin = 5\n",
    "param.stdmax = 50\n",
    "param.ref_ms = 1.5\n",
    "param.preprocessing = True\n",
    "param.minus_one = 0\n",
    "param.interpolation = 'n'\n",
    "param.segments_length = 5\n",
    "param.tmax = 'all'\n",
    "param.cont_plot_samples = 60000\n",
    "\n",
    "# ========================================\n",
    "# RUN SPIKE DETECTION\n",
    "# ========================================\n",
    "\n",
    "print('\\nStarting spike detection...')\n",
    "param.detection = 'neg'\n",
    "\n",
    "if param.parallel:\n",
    "    spike = Spikes(par=param, nsx=nsx)\n",
    "    \n",
    "    # Process negative detection channels\n",
    "    if neg_thr_channels.size:\n",
    "        print(f\"Processing {len(neg_thr_channels)} channels with negative detection...\")\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "            pool.map(spike.get_spikes, neg_thr_channels, chunksize=1)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    \n",
    "    # Process positive detection channels\n",
    "    param.detection = 'pos'\n",
    "    if pos_thr_channels.size:\n",
    "        print(f\"Processing {len(pos_thr_channels)} channels with positive detection...\")\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "            pool.map(spike.get_spikes, pos_thr_channels, chunksize=1)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    \n",
    "    # Process 'both' detection channels\n",
    "    param.detection = 'both'\n",
    "    both_thr_channels = np.setdiff1d(\n",
    "        np.setdiff1d(\n",
    "            np.array([ch[0] for ch in channels]),\n",
    "            np.array([ch[0] for ch in neg_thr_channels])\n",
    "        ),\n",
    "        np.array([ch[0] for ch in pos_thr_channels]) if pos_thr_channels.size else []\n",
    "    )\n",
    "    \n",
    "    if both_thr_channels.size:\n",
    "        # Convert back to channel objects\n",
    "        both_thr_channels = np.array([ch for ch in channels if ch[0] in both_thr_channels], dtype=object)\n",
    "        print(f\"Processing {len(both_thr_channels)} channels with both detection...\")\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "            pool.map(spike.get_spikes, both_thr_channels, chunksize=1)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    \n",
    "    print('Spike detection done!')\n",
    "    \n",
    "else:\n",
    "    spike = Spikes(par=param, nsx=nsx)\n",
    "    \n",
    "    # Process negative detection channels\n",
    "    if neg_thr_channels.size:\n",
    "        print(f\"Processing {len(neg_thr_channels)} channels with negative detection...\")\n",
    "        for channel in neg_thr_channels:\n",
    "            spike.get_spikes(channel)\n",
    "    \n",
    "    # Process positive detection channels\n",
    "    param.detection = 'pos'\n",
    "    if pos_thr_channels.size:\n",
    "        print(f\"Processing {len(pos_thr_channels)} channels with positive detection...\")\n",
    "        for channel in pos_thr_channels:\n",
    "            spike.get_spikes(channel)\n",
    "    \n",
    "    # Process 'both' detection channels\n",
    "    param.detection = 'both'\n",
    "    both_thr_channels = np.setdiff1d(\n",
    "        np.setdiff1d(\n",
    "            np.array([ch[0] for ch in channels]),\n",
    "            np.array([ch[0] for ch in neg_thr_channels])\n",
    "        ),\n",
    "        np.array([ch[0] for ch in pos_thr_channels]) if pos_thr_channels.size else []\n",
    "    )\n",
    "    \n",
    "    if both_thr_channels.size:\n",
    "        # Convert back to channel objects\n",
    "        both_thr_channels = np.array([ch for ch in channels if ch[0] in both_thr_channels], dtype=object)\n",
    "        print(f\"Processing {len(both_thr_channels)} channels with both detection...\")\n",
    "        for channel in both_thr_channels:\n",
    "            spike.get_spikes(channel)\n",
    "    \n",
    "    print('Spike detection done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collisions for all 68 channels: [array([257], dtype=uint16), array([258], dtype=uint16), array([259], dtype=uint16), array([260], dtype=uint16), array([261], dtype=uint16), array([262], dtype=uint16), array([263], dtype=uint16), array([264], dtype=uint16), array([266], dtype=uint16), array([267], dtype=uint16), array([268], dtype=uint16), array([269], dtype=uint16), array([270], dtype=uint16), array([271], dtype=uint16), array([272], dtype=uint16), array([273], dtype=uint16), array([289], dtype=uint16), array([290], dtype=uint16), array([291], dtype=uint16), array([292], dtype=uint16), array([293], dtype=uint16), array([294], dtype=uint16), array([295], dtype=uint16), array([296], dtype=uint16), array([298], dtype=uint16), array([299], dtype=uint16), array([300], dtype=uint16), array([301], dtype=uint16), array([302], dtype=uint16), array([303], dtype=uint16), array([304], dtype=uint16), array([305], dtype=uint16), array([321], dtype=uint16), array([322], dtype=uint16), array([323], dtype=uint16), array([324], dtype=uint16), array([325], dtype=uint16), array([326], dtype=uint16), array([327], dtype=uint16), array([328], dtype=uint16), array([330], dtype=uint16), array([331], dtype=uint16), array([332], dtype=uint16), array([333], dtype=uint16), array([334], dtype=uint16), array([335], dtype=uint16), array([336], dtype=uint16), array([337], dtype=uint16), array([353], dtype=uint16), array([354], dtype=uint16), array([355], dtype=uint16), array([356], dtype=uint16), array([357], dtype=uint16), array([358], dtype=uint16), array([359], dtype=uint16), array([360], dtype=uint16), array([362], dtype=uint16), array([363], dtype=uint16), array([364], dtype=uint16), array([365], dtype=uint16), array([366], dtype=uint16), array([367], dtype=uint16), array([368], dtype=uint16), array([369], dtype=uint16), array([10241], dtype=uint16), array([10242], dtype=uint16), array([10245], dtype=uint16), array([10246], dtype=uint16)]\n",
      "Collision detection (['mLAMY']). Total artifacts:5489/144931(3.79%) \n",
      "\n",
      "Collision detection (['mLFUS']). Total artifacts:764/135179(0.57%) \n",
      "\n",
      "Collision detection (['mLHIPH']). Total artifacts:713/160612(0.44%) \n",
      "\n",
      "Collision detection (['mLTP']). Total artifacts:2086/70147(2.97%) \n",
      "\n",
      "Collision detection (['mRAMY']). Total artifacts:18104/154830(11.69%) \n",
      "\n",
      "Collision detection (['mRFUS']). Total artifacts:332/170907(0.19%) \n",
      "\n",
      "Collision detection (['mRHIPH']). Total artifacts:718/43226(1.66%) \n",
      "\n",
      "Collision detection (['mRTP']). Total artifacts:1657/38819(4.27%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = Collision(all_channels,nsx)\n",
    "\n",
    "# To select specific channels:\n",
    "specific_channels = 'all'  # Use all channels\n",
    "# specific_channels = [257, 263]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "col.separate_collisions(channels=specific_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected channels: [257, 263, 301]\n",
      "Found 3 channels to process\n",
      "Thresholds: Prom>2.0, Width>3.0 & <15.0\n",
      "Low Amp Percentile: 5.0%, Other Prom Min: 1.0%\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing channel 257: mLTP01 raw_257_spikes.mat\n",
      "============================================================\n",
      "Loaded 24007 spikes (using spikes_all)\n",
      "Calculating metrics for 24007 spikes...\n",
      "  Using 80 CPU cores for parallel processing...\n",
      "Applying artifact rejection logic...\n",
      "  Amplitude threshold (5.0th percentile): 22.06\n",
      "  Auto-quarantined (bad ratio/width): 81\n",
      "  Low amplitude: 1201\n",
      "  One-peak exceptions preserved: 23\n",
      "\n",
      "Results:\n",
      "  Total spikes: 24007\n",
      "  Preserved: 19455 (81.0%)\n",
      "  Quarantined: 4462 (18.6%)\n",
      "  Bundle collision: 296 (1.2%)\n",
      "  Both quarantined & bundle: 206\n",
      "Saved updated file to: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/mLTP01 raw_257_spikes.mat\n",
      "\n",
      "============================================================\n",
      "Processing channel 263: mLTP07 raw_263_spikes.mat\n",
      "============================================================\n",
      "Loaded 7425 spikes (using spikes_all)\n",
      "Calculating metrics for 7425 spikes...\n",
      "  Using 80 CPU cores for parallel processing...\n",
      "Applying artifact rejection logic...\n",
      "  Amplitude threshold (5.0th percentile): 16.28\n",
      "  Auto-quarantined (bad ratio/width): 103\n",
      "  Low amplitude: 372\n",
      "  One-peak exceptions preserved: 30\n",
      "\n",
      "Results:\n",
      "  Total spikes: 7425\n",
      "  Preserved: 4869 (65.6%)\n",
      "  Quarantined: 2437 (32.8%)\n",
      "  Bundle collision: 351 (4.7%)\n",
      "  Both quarantined & bundle: 232\n",
      "Saved updated file to: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/mLTP07 raw_263_spikes.mat\n",
      "\n",
      "============================================================\n",
      "Processing channel 301: mLHIPH04 raw_301_spikes.mat\n",
      "============================================================\n",
      "Loaded 1492 spikes (using spikes_all)\n",
      "Calculating metrics for 1492 spikes...\n",
      "  Using 80 CPU cores for parallel processing...\n",
      "Applying artifact rejection logic...\n",
      "  Amplitude threshold (5.0th percentile): 14.55\n",
      "  Auto-quarantined (bad ratio/width): 72\n",
      "  Low amplitude: 73\n",
      "  One-peak exceptions preserved: 16\n",
      "\n",
      "Results:\n",
      "  Total spikes: 1492\n",
      "  Preserved: 635 (42.6%)\n",
      "  Quarantined: 835 (56.0%)\n",
      "  Bundle collision: 126 (8.4%)\n",
      "  Both quarantined & bundle: 104\n",
      "Saved updated file to: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/mLHIPH04 raw_301_spikes.mat\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "artifact_rej = ArtifactRejection(\n",
    "    input_dir=str(Path().resolve() / 'output'),\n",
    "    output_dir=None,  # None = same as input_dir\n",
    "    backup_dir=None   # None = input_dir/backup\n",
    ")\n",
    "\n",
    "# Set thresholds (optional - defaults shown)\n",
    "artifact_rej.set_thresholds(\n",
    "    prom_ratio=2.0,           # Prominence ratio threshold\n",
    "    width_upper=15.0,         # Upper width limit\n",
    "    width_lower=3.0,          # Lower width limit\n",
    "    low_amp_percentile=5.0,   # Low amplitude percentile\n",
    "    other_prom_min_ratio=0.01 # 1% rule for other prominence\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# CHANNEL SELECTION\n",
    "# ========================================\n",
    "\n",
    "# To select specific channels:\n",
    "# specific_channels = 'all'  # Use all channels\n",
    "specific_channels = [257, 263, 301]  # Use specific channels\n",
    "# specific_channels = 290  # Use single channel\n",
    "# specific_channels = list(range(290, 301))  # Use range 290-300 (inclusive)\n",
    "\n",
    "# ========================================\n",
    "# PROCESS ARTIFACT REJECTION\n",
    "# ========================================\n",
    "\n",
    "artifact_rej.process_all_channels(channels=specific_channels, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "FEATURE EXTRACTION PIPELINE\n",
      "######################################################################\n",
      "Method: GMM\n",
      "Input directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/features\n",
      "Channels: [301, 304]\n",
      "Total channels: 2\n",
      "Scales: 4, Min weight: 0.005, Min coeff: 5\n",
      "######################################################################\n",
      "\n",
      "[1/2] Processing channel 301...\n",
      "\n",
      "============================================================\n",
      "Channel 301 - GMM extraction\n",
      "============================================================\n",
      "Loaded 635 spikes (shape: (635, 64))\n",
      "  Running GMM_1channel (scales=4)...\n",
      "  Processed 64 coefficients\n",
      "  Filtering components (min_weight=0.005)...\n",
      "  Expanding metrics into vectors...\n",
      "  Applying knee detection...\n",
      "  Selecting final coefficients...\n",
      "  Selected 10 coefficients\n",
      "Saved features to: ch301_features_gmm.mat\n",
      "Feature matrix shape: (635, 10)\n",
      "[2/2] Processing channel 304...\n",
      "\n",
      "============================================================\n",
      "Channel 304 - GMM extraction\n",
      "============================================================\n",
      "Loaded 2097 spikes (shape: (2097, 64))\n",
      "  Running GMM_1channel (scales=4)...\n",
      "  Processed 64 coefficients\n",
      "  Filtering components (min_weight=0.005)...\n",
      "  Expanding metrics into vectors...\n",
      "  Applying knee detection...\n",
      "  Selecting final coefficients...\n",
      "  Selected 10 coefficients\n",
      "Saved features to: ch304_features_gmm.mat\n",
      "Feature matrix shape: (2097, 10)\n",
      "\n",
      "######################################################################\n",
      "PROCESSING COMPLETE\n",
      "######################################################################\n",
      "Successful: 2/2\n",
      "Failed: 0/2\n",
      "Output files: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/features\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define input/output directories\n",
    "input_dir = str(Path().resolve() / 'output')\n",
    "output_dir = str(Path().resolve() / 'output' / 'features')\n",
    "\n",
    "# GMM (recommended)\n",
    "extractor = FeatureExtractor(input_dir, output_dir, method='gmm')\n",
    "#extractor.process_all_channels(channels='all')\n",
    "extractor.process_all_channels(channels=[304, 301])\n",
    "\n",
    "# # Haar (fast)\n",
    "# extractor = FeatureExtractor(input_dir, output_dir, method='haar')\n",
    "# extractor.process_all_channels(channels=[257, 263])\n",
    "\n",
    "# # PCA\n",
    "# extractor = FeatureExtraction(input_dir, output_dir, method='pca', n_components=10)\n",
    "# extractor.process_all_channels(channels=list(range(290, 301)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "CLUSTERING PIPELINE\n",
      "######################################################################\n",
      "Input directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/features\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/features\n",
      "Times output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Channels: [257, 304, 328]\n",
      "Total channels: 3\n",
      "Parameters:\n",
      "  Min cluster size: 20\n",
      "  Max clusters: 20\n",
      "  Temperature: 0.15\n",
      "  K-nearest neighbors: 11\n",
      "  Generate times files: True\n",
      "  Primary method: K-means\n",
      "  Fallback method: WaveClus SPC\n",
      "######################################################################\n",
      "\n",
      "[1/3] Processing channel 257...\n",
      "\n",
      "============================================================\n",
      "Channel 257 - Clustering\n",
      "File: ch257_features_gmm.mat (feature file)\n",
      "============================================================\n",
      "Loaded 19455 spikes with 10 features\n",
      "  Standardizing features...\n",
      "  Testing k=2 to k=20...\n",
      "    k=2: silhouette=0.538, DB=0.969\n",
      "    k=3: silhouette=0.242, DB=1.486\n",
      "    k=4: silhouette=0.165, DB=1.674\n",
      "    k=5: silhouette=0.156, DB=1.640\n",
      "    k=6: silhouette=0.134, DB=1.692\n",
      "    k=7: silhouette=0.122, DB=1.741\n",
      "    k=8: silhouette=0.115, DB=1.722\n",
      "    k=9: silhouette=0.113, DB=1.758\n",
      "    k=10: silhouette=0.112, DB=1.785\n",
      "    k=11: silhouette=0.110, DB=1.702\n",
      "    k=12: silhouette=0.103, DB=1.795\n",
      "    k=13: silhouette=0.104, DB=1.755\n",
      "    k=14: silhouette=0.103, DB=1.723\n",
      "    k=15: silhouette=0.106, DB=1.647\n",
      "    k=16: silhouette=0.101, DB=1.639\n",
      "    k=17: silhouette=0.098, DB=1.647\n",
      "    k=18: silhouette=0.098, DB=1.694\n",
      "    k=19: silhouette=0.095, DB=1.749\n",
      "    k=20: silhouette=0.099, DB=1.635\n",
      "  ✓ Best k=2 with silhouette=0.538\n",
      "\n",
      "  Final result:\n",
      "    2 clusters, 0 noise spikes\n",
      "    Cluster 1: 1281 spikes (6.6%)\n",
      "    Cluster 2: 18174 spikes (93.4%)\n",
      "Saved clustering results to: ch257_features_gmm.mat\n",
      "Clusters: 2\n",
      "  Cluster 1: 1281 spikes\n",
      "  Cluster 2: 18174 spikes\n",
      "Generating times file...\n",
      "  ✓ Times file created: times_ch257.mat\n",
      "[2/3] Processing channel 304...\n",
      "\n",
      "============================================================\n",
      "Channel 304 - Clustering\n",
      "File: ch304_features_gmm.mat (feature file)\n",
      "============================================================\n",
      "Loaded 2097 spikes with 10 features\n",
      "  Standardizing features...\n",
      "  Testing k=2 to k=20...\n",
      "    k=2: silhouette=0.407, DB=1.444\n",
      "    k=3: silhouette=0.123, DB=1.972\n",
      "    k=4: silhouette=0.126, DB=1.842\n",
      "    k=5: silhouette=0.129, DB=1.770\n",
      "    k=6: silhouette=0.108, DB=1.814\n",
      "    k=7: silhouette=0.108, DB=1.701\n",
      "    k=8: silhouette=0.113, DB=1.678\n",
      "    k=9: silhouette=0.101, DB=1.655\n",
      "    k=10: silhouette=0.094, DB=1.663\n",
      "    k=11: silhouette=0.110, DB=1.661\n",
      "    k=12: silhouette=0.096, DB=1.696\n",
      "    k=13: silhouette=0.088, DB=1.671\n",
      "    k=14: silhouette=0.091, DB=1.617\n",
      "    k=15: silhouette=0.088, DB=1.594\n",
      "    k=16: silhouette=0.087, DB=1.618\n",
      "    k=17: silhouette=0.086, DB=1.626\n",
      "    k=18: silhouette=0.080, DB=1.681\n",
      "    k=19: silhouette=0.083, DB=1.529\n",
      "    k=20: silhouette=0.090, DB=1.609\n",
      "  ✓ Best k=2 with silhouette=0.407\n",
      "\n",
      "  Final result:\n",
      "    2 clusters, 0 noise spikes\n",
      "    Cluster 1: 1802 spikes (85.9%)\n",
      "    Cluster 2: 295 spikes (14.1%)\n",
      "Saved clustering results to: ch304_features_gmm.mat\n",
      "Clusters: 2\n",
      "  Cluster 1: 1802 spikes\n",
      "  Cluster 2: 295 spikes\n",
      "Generating times file...\n",
      "  ✓ Times file created: times_ch304.mat\n",
      "[3/3] Processing channel 328...\n",
      "\n",
      "============================================================\n",
      "Channel 328 - Clustering\n",
      "File: ch328_features_gmm.mat (feature file)\n",
      "============================================================\n",
      "Loaded 82 spikes with 10 features\n",
      "  Standardizing features...\n",
      "  Testing k=2 to k=4...\n",
      "    k=2: silhouette=0.138, DB=2.101\n",
      "    k=3: silhouette=0.134, DB=1.924\n",
      "    k=4: silhouette=0.138, DB=1.487\n",
      "  ✓ Best k=2 with silhouette=0.138\n",
      "\n",
      "  Final result:\n",
      "    2 clusters, 0 noise spikes\n",
      "    Cluster 1: 38 spikes (46.3%)\n",
      "    Cluster 2: 44 spikes (53.7%)\n",
      "Saved clustering results to: ch328_features_gmm.mat\n",
      "Clusters: 2\n",
      "  Cluster 1: 38 spikes\n",
      "  Cluster 2: 44 spikes\n",
      "Generating times file...\n",
      "  ✓ Times file created: times_ch328.mat\n",
      "\n",
      "######################################################################\n",
      "CLUSTERING COMPLETE\n",
      "######################################################################\n",
      "Successful: 3/3\n",
      "Failed: 0/3\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/features\n",
      "Times files saved to: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cluster all channels in feature directory\n",
    "clusterer = SPCClustering(\n",
    "    input_dir=str(Path().resolve() / 'output' / 'features'),\n",
    "    times_output_dir=str(Path().resolve() / 'output'),  # Explicitly set output dir\n",
    "    min_clus=20,\n",
    "    temperature=0.15,\n",
    "    knn=11,\n",
    "    generate_times_files=True,  # Explicitly enable\n",
    "    verbose=True  # Show all logs\n",
    ")\n",
    "#clusterer.process_all_channels(channels='all')\n",
    "\n",
    "# Cluster specific channels\n",
    "clusterer.process_all_channels(channels=[304, 328, 257]) #328, 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "TEMPLATE MATCHING PIPELINE\n",
      "######################################################################\n",
      "Input directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Total channels to process: 1\n",
      "Parameters: {'template_sdnum': 3.0, 'template_k': None, 'template_k_min': 1, 'use_pointdist': False, 'pointlimit': None, 'metric': 'euclidean'}\n",
      "######################################################################\n",
      "\n",
      "\n",
      "============================================================\n",
      "Channel 257 - Template Matching\n",
      "File: times_ch257.mat\n",
      "============================================================\n",
      "Skipping: No unclassified spikes (label 0) found to match.\n",
      "\n",
      "######################################################################\n",
      "TEMPLATE MATCHING COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the matcher\n",
    "matcher = TemplateMatcher(\n",
    "                    input_dir=str(Path().resolve() / 'output'),\n",
    "                    template_sdnum=3.0,\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "                \n",
    "## Match unclassified spikes in ALL channels\n",
    "# matcher.process_all_channels(channels='all')\n",
    "                \n",
    "# Match unclassified spikes in specific channels\n",
    "matcher.process_all_channels(channels=[257]) #328, 301\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "SPIKE QUALITY METRICS PIPELINE\n",
      "######################################################################\n",
      "Input directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/Quality_Metrics\n",
      "Total channels to process: 1\n",
      "Parameters: {'exclude_cluster_0': True, 'n_jobs': -1, 'n_neighbors': 5, 'bin_duration_ms': 60000.0, 'refractory_period_ms': 3.0, 'censored_period_ms': 0.0}\n",
      "######################################################################\n",
      "\n",
      "\n",
      "============================================================\n",
      "Channel 304 - Computing Quality Metrics\n",
      "File: times_ch304.mat\n",
      "============================================================\n",
      "  Using 'features' field for features\n",
      "Computing ALL metrics for 2 clusters with parallelism (n_jobs=-1)...\n",
      "\n",
      "✓ All metrics computed successfully!\n",
      "✓ Successfully computed metrics for 2 clusters.\n",
      "\n",
      "######################################################################\n",
      "METRICS COMPUTATION COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "✓ Aggregated metrics saved to: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output/Quality_Metrics/spike_quality_metrics.csv\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>firing_rate</th>\n",
       "      <th>snr</th>\n",
       "      <th>presence_ratio</th>\n",
       "      <th>amplitude_cutoff</th>\n",
       "      <th>cv2</th>\n",
       "      <th>isi_violation_rate</th>\n",
       "      <th>isi_contamination_fraction</th>\n",
       "      <th>isi_violations_count</th>\n",
       "      <th>isolation_distance</th>\n",
       "      <th>l_ratio</th>\n",
       "      <th>d_prime</th>\n",
       "      <th>nearest_neighbor_hit_rate</th>\n",
       "      <th>nearest_neighbor_miss_rate</th>\n",
       "      <th>silhouette_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304</td>\n",
       "      <td>1</td>\n",
       "      <td>1802</td>\n",
       "      <td>0.846030</td>\n",
       "      <td>3.033591</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012594</td>\n",
       "      <td>1.057666</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>4.263564</td>\n",
       "      <td>39</td>\n",
       "      <td>806.646443</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>2.657019</td>\n",
       "      <td>0.980022</td>\n",
       "      <td>0.115254</td>\n",
       "      <td>0.548308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304</td>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "      <td>0.138501</td>\n",
       "      <td>2.926502</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.105983</td>\n",
       "      <td>0.813157</td>\n",
       "      <td>0.007910</td>\n",
       "      <td>57.108635</td>\n",
       "      <td>14</td>\n",
       "      <td>7.674519</td>\n",
       "      <td>2.066880</td>\n",
       "      <td>2.657019</td>\n",
       "      <td>0.884746</td>\n",
       "      <td>0.019978</td>\n",
       "      <td>0.548308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   channel_id  cluster_id  num_spikes  firing_rate       snr  presence_ratio  \\\n",
       "0         304           1        1802     0.846030  3.033591        1.000000   \n",
       "1         304           2         295     0.138501  2.926502        0.305556   \n",
       "\n",
       "   amplitude_cutoff       cv2  isi_violation_rate  isi_contamination_fraction  \\\n",
       "0          0.012594  1.057666            0.003607                    4.263564   \n",
       "1          0.105983  0.813157            0.007910                   57.108635   \n",
       "\n",
       "   isi_violations_count  isolation_distance   l_ratio   d_prime  \\\n",
       "0                    39          806.646443  0.007232  2.657019   \n",
       "1                    14            7.674519  2.066880  2.657019   \n",
       "\n",
       "   nearest_neighbor_hit_rate  nearest_neighbor_miss_rate  silhouette_score  \n",
       "0                   0.980022                    0.115254          0.548308  \n",
       "1                   0.884746                    0.019978          0.548308  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_calc = SpikeMetricsCalculator(\n",
    "        input_dir=str(Path().resolve() / 'output'),\n",
    "        output_dir=str(Path().resolve() / 'output/Quality_Metrics')\n",
    "    )\n",
    "    \n",
    "## Run on all channels\n",
    "#metrics_calc.process_all_channels(channels='all')\n",
    "    \n",
    "## Run on specific channels\n",
    "metrics_calc.process_all_channels(channels=[304])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Spike Rescue on specific channels ---\n",
      "\n",
      "######################################################################\n",
      "SPIKE RESCUE PIPELINE - FIXED VERSION\n",
      "######################################################################\n",
      "Times (Data) Dir: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Spikes (Rescue) Dir: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Output Dir: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Total channels to process: 1\n",
      "Rescue threshold: Label <= 0\n",
      "Parameters: {'template_sdnum': 3.0, 'metric': 'euclidean'}\n",
      "######################################################################\n",
      "\n",
      "[1/1] Processing channel 257...\n",
      "\n",
      "============================================================\n",
      "Channel 257 - Spike Rescue\n",
      "  Templates File: times_ch257.mat\n",
      "  Spikes Source:  mLTP01 raw_257_spikes.mat\n",
      "============================================================\n",
      "Loaded 19455 classified spikes from times file.\n",
      "  Using 'spikes_all' field (24007 total detected spikes)\n",
      "  Using 'index_all' field (24007 times)\n",
      "\n",
      "  Comparing spike times to find quarantined spikes...\n",
      "  Total spikes in spike file: 24007\n",
      "  Already classified in times file: 19455\n",
      "  Quarantined (rescue candidates): 4552 (19.0%)\n",
      "  No features found in spike file (optional)\n",
      "\n",
      "  Ready to rescue 4552 spikes.\n",
      "\n",
      "Template Matching Parameters:\n",
      "  template_sdnum: 3.0\n",
      "  metric: euclidean\n",
      "\n",
      "Filtering rescue candidates:\n",
      "  Total rescue candidates: 4552\n",
      "  Already classified: 0\n",
      "  Truly unclassified: 4552\n",
      "\n",
      "Running template matching on 4552 spikes...\n",
      "\n",
      "Rescue Results:\n",
      "  Total matched to templates: 4252\n",
      "  Remain unclassified: 300\n",
      "\n",
      "  Rescued spikes by cluster:\n",
      "    Cluster 1: 451 spikes\n",
      "    Cluster 2: 3801 spikes\n",
      "\n",
      "✓ Successfully rescued 4552 spikes for channel 257.\n",
      "✓ Updated file saved to: times_ch257.mat\n",
      "\n",
      "######################################################################\n",
      "SPIKE RESCUE COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Define the directories\n",
    "# This assumes your notebook is in 'full_processing_pipeline'\n",
    "base_dir = Path().resolve() \n",
    "times_dir = str(base_dir / 'output')\n",
    "spikes_dir = str(base_dir / 'output')\n",
    "\n",
    "# 3. Initialize the SpikeRescuer\n",
    "# You can choose 'euclidean' or 'correlation'\n",
    "rescuer = SpikeRescuer(\n",
    "    times_dir=times_dir,\n",
    "    spikes_dir=spikes_dir,\n",
    "    output_dir=times_dir,  # This will update the times_*.mat files in-place\n",
    "    template_sdnum=3.0,    # Use 3.0 for 'euclidean'\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "# --- 4. Run the rescue operation ---\n",
    "\n",
    "# Option A: Run on ALL channels\n",
    "# This will find all channels that have both a 'times_*.mat'\n",
    "# and a '*_spikes.mat' file and process them.\n",
    "# print(\"--- Running Spike Rescue on ALL channels ---\")\n",
    "#rescuer.process_all_channels(channels='all')\n",
    "\n",
    "\n",
    "# Option B: Run on specific channels\n",
    "print(\"\\n--- Running Spike Rescue on specific channels ---\")\n",
    "specific_channels = [257]\n",
    "rescuer.process_all_channels(channels=specific_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "CLUSTER MERGING & REPORTING PIPELINE\n",
      "######################################################################\n",
      "Input directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output\n",
      "Output MAT directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output_merged_mat\n",
      "Output PDF directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output_merged_reports\n",
      "Total channels to process: 1\n",
      "Merge Groups: [[1, 3], [2, 5, 7]]\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Channel 257 - Cluster Merging\n",
      "File: times_ch257.mat\n",
      "======================================================================\n",
      "\n",
      "=� Original cluster distribution:\n",
      "   Cluster 0:   300 spikes (  1.2%)\n",
      "   Cluster 1:  1732 spikes (  7.2%)\n",
      "   Cluster 2: 21975 spikes ( 91.5%)\n",
      "\n",
      "= Applying merges...\n",
      "\n",
      "   Merging [1, 3] � 1\n",
      "     Cluster 3 not found (0 spikes)\n",
      "   Total merged into cluster 1: 0 spikes\n",
      "\n",
      "   Merging [2, 5, 7] � 2\n",
      "     Cluster 5 not found (0 spikes)\n",
      "     Cluster 7 not found (0 spikes)\n",
      "   Total merged into cluster 2: 0 spikes\n",
      "\n",
      "= Renumbering clusters to be contiguous (0 to N)...\n",
      "   Cluster mapping (old � new):\n",
      "       0 �   0   (  300 spikes)\n",
      "       1 �   1   ( 1732 spikes)\n",
      "       2 �   2   (21975 spikes)\n",
      "\n",
      "= Final cluster distribution:\n",
      "   Cluster 0:   300 spikes (  1.2%)\n",
      "   Cluster 1:  1732 spikes (  7.2%)\n",
      "   Cluster 2: 21975 spikes ( 91.5%)\n",
      "\n",
      " Cluster count: 3 � 3\n",
      "\n",
      "Saved merged data to: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output_merged_mat/times_ch257_merged.mat\n",
      "\n",
      "=� Generating cluster report for merged data...\n",
      "--- FAKE make_cluster_report ---\n",
      "   No figures generated by report.\n",
      "\n",
      "######################################################################\n",
      "MERGE & REPORT COMPLETE\n",
      "######################################################################\n",
      "Successful: 1/1\n",
      "Failed: 0/1\n",
      "Output directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output_merged_mat\n",
      "Report directory: /home/tapasib/MCWs/Python_Pipeline/full_processing_pipeline/output_merged_reports\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ClusterMerger\n",
    "merger = ClusterMerger(\n",
    "    input_dir=str(Path().resolve() / 'output'),\n",
    "    output_dir=str(Path().resolve() / 'output_merged_mat'),\n",
    "    report_dir=str(Path().resolve() / 'output_merged_reports'),\n",
    "    calc_metrics=True,\n",
    "    verbose=True,\n",
    "    clusters_per_page=6  # Optional: kwargs for make_cluster_report\n",
    ")\n",
    "\n",
    "# Define merge groups (optional)\n",
    "# Each sublist: [target_cluster, source_cluster1, source_cluster2, ...]\n",
    "merge_rules = [\n",
    "    [1, 3],    # Merge cluster 3 into cluster 1\n",
    "    [2, 5, 7]  # Merge clusters 5 and 7 into cluster 2\n",
    "]\n",
    "\n",
    "# Run on all channels\n",
    "# merger.process_all_channels(channels='all', merge_groups=merge_rules)\n",
    "\n",
    "# Run on specific channels with merging\n",
    "merger.process_all_channels(channels=[257], merge_groups=merge_rules)\n",
    "\n",
    "# Or run without merging (just renumber clusters)\n",
    "# merger.process_all_channels(channels=[328], merge_groups=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
