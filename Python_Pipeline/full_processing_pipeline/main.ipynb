{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from core.parameter_functions.Parameters import par\n",
    "from scipy.io import loadmat\n",
    "import sys ; sys.path.append(os.path.abspath(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) + '/codes_emu/codes_for_analysis/new_processing_pipeline')\n",
    "from parse_ripple import parse_ripple\n",
    "from core.filtering import filtering\n",
    "from core.plot_continuous_bundles import PlotBundles\n",
    "from core.spikeDetection import Spikes\n",
    "from core.Collision import Collision\n",
    "from core.Clustering import clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = par()\n",
    "param.parallel = True\n",
    "if param.micros:\n",
    "    binary_ext = '.NC5'\n",
    "    ext = '.ns5'\n",
    "else:\n",
    "    binary_ext = '.NC3'\n",
    "    ext = '.nf3'\n",
    "file_paths = glob.glob(\"input/*\"+binary_ext)\n",
    "if file_paths == []:\n",
    "    file_paths_ns5 = glob.glob(\"input/*\"+ext)\n",
    "    parse_ripple(file_paths_ns5)\n",
    "    file_paths = glob.glob(\"input/*\"+binary_ext)\n",
    "NSx_file_path = os.path.abspath(glob.glob(\"input/NSx.mat\")[0])\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "pics_used_dir = dir_path + '/input/pics_used'\n",
    "metadata = loadmat(NSx_file_path)\n",
    "nsx = metadata['NSx']\n",
    "if param.micros:\n",
    "    channels = nsx['chan_ID'][0][list(set(np.where(nsx['unit']=='uV')[1]) & set(np.where(nsx['sr']==30000)[1]))]\n",
    "else:\n",
    "    channels = nsx['chan_ID'][0][list(set(np.where(nsx['sr']==2000)[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = filtering(save_fig=False,show_img=True,direc_resus_bae=os.path.dirname(os.path.realpath(__file__)),\n",
    "                    resus_folder_name='spectra',direc_raw=os.path.dirname(os.path.realpath(__file__)),with_NoNotch = False,\n",
    "                    time_plot_duration = 1,freq_line=60,parallel=param.parallel,k_periodograms=200,notch_filter=True,spectrum_resolution=0.5)\n",
    "filter.new_check_lfp_power_NSX(metadata, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = PlotBundles()\n",
    "plt.plot(nsx_file = nsx, par = param,notchfilter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if param.micros:\n",
    "    ch_temp = []\n",
    "    if param.fast_analysis or param.nowait:\n",
    "        neg_thr_channels = channels\n",
    "        pos_thr_channels = []\n",
    "    else:\n",
    "        ch_temp = input(f'Currently, Channels = {channels}. \\nIf you want to keep it like that, press enter.\\nOtherwise, enter the new vector and press enter ')\n",
    "    if ch_temp !='':\n",
    "        channels = ch_temp\n",
    "    \n",
    "    neg_thr_channels = input('Enter the vector with neg_thr_channels and press enter. Press enter to use all channels ')\n",
    "    if ch_temp == '':\n",
    "        neg_thr_channels = channels\n",
    "        pos_thr_channels = np.array([])\n",
    "    else:\n",
    "        pos_thr_channels = input('Enter the vector with pos_thr_channels and press enter. Press enter for empty array ')\n",
    "\n",
    "    #start parallel process\n",
    "    del param\n",
    "    param = par()\n",
    "    param.detection = 'neg'\n",
    "    param.sr = 30000\n",
    "    param.detect_fmin = 300\n",
    "    param.detect_fmax = 3000\n",
    "    param.auto = 0\n",
    "    param.mVmin = 50\n",
    "    param.w_pre=20                       \n",
    "    param.w_post=44                     \n",
    "    param.min_ref_per=1.5                                    \n",
    "    param.ref = np.floor(param.min_ref_per*param.sr/1000)                  \n",
    "    param.ref = param.ref\n",
    "    param.factor_thr=5\n",
    "    param.detect_order = 4\n",
    "    param.sort_order = 2\n",
    "    param.detect_fmin = 300\n",
    "    param.sort_fmin = 300\n",
    "    param.stdmin = 5\n",
    "    param.stdmax = 50\n",
    "    param.ref_ms = 1.5\n",
    "    param.preprocessing = True\n",
    "    param.minus_one = 0\n",
    "    print('starting spike detection')\n",
    "    param.detection = 'neg'\n",
    "    if param.parallel:\n",
    "        spike = Spikes(par=param,nsx=nsx)\n",
    "        if neg_thr_channels.size:\n",
    "            with multiprocessing.Pool(processes=10) as pool:\n",
    "                pool.imap(spike.get_spikes,neg_thr_channels,chunksize=10)#might need to use imap()\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "        param.detection = 'pos'\n",
    "        if pos_thr_channels.size:\n",
    "            with multiprocessing.Pool(processes=10) as pool:\n",
    "                pool.imap(spike.get_spikes,pos_thr_channels,chunksize=10)#might need to use imap()\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                \n",
    "        param.detection ='both'\n",
    "        both_thr_channels = np.setdiff1d(np.setdiff1d(channels,neg_thr_channels),pos_thr_channels)\n",
    "        if both_thr_channels.size:\n",
    "            with multiprocessing.Pool(processes=10) as pool:\n",
    "                pool.imap(spike.get_spikes,both_thr_channels,chunksize=10)#might need to use imap()\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                \n",
    "                \n",
    "        print('spike detection done')\n",
    "        \n",
    "    else:\n",
    "        if neg_thr_channels.size:\n",
    "            spike = Spikes(par=param,nsx=nsx)\n",
    "            for channel in neg_thr_channels:\n",
    "                spike.get_spikes(channel=channel[0][0])\n",
    "        param.detection = 'pos'\n",
    "        if pos_thr_channels.size:\n",
    "            for channel in pos_thr_channels:\n",
    "                spike.get_spikes(channel=channel[0][0])\n",
    "        param.detection ='both'\n",
    "        both_thr_channels = np.setdiff1d(np.setdiff1d(channels,neg_thr_channels),pos_thr_channels)\n",
    "        if not both_thr_channels.size:\n",
    "            for channel in both_thr_channels:\n",
    "                spike.get_spikes(channel[0][0])\n",
    "        print('spike detection done')\n",
    "    col = Collision(channels,nsx)\n",
    "    col.separate_collisions()\n",
    "    clus = clustering()\n",
    "    clus.do_clustering(par,nsx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
